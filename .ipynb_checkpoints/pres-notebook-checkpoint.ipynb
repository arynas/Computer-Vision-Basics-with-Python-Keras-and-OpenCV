{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV Version: 4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad value in file '/Users/arynas/.matplotlib/matplotlibrc', line 1 ('backend: TkAggy'): Key backend: 'tkaggy' is not a valid value for backend; supported values are ['GTK3Agg', 'GTK3Cairo', 'GTK4Agg', 'GTK4Cairo', 'MacOSX', 'nbAgg', 'QtAgg', 'QtCairo', 'Qt5Agg', 'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras image data format: channels_last\n"
     ]
    }
   ],
   "source": [
    "import sys # system functions (ie. exiting the program)\n",
    "import os # operating system functions (ie. path building on Windows vs. MacOs)\n",
    "import time # for time operations\n",
    "import uuid # for generating unique file names\n",
    "import math # math functions\n",
    "\n",
    "from IPython.display import display as ipydisplay, Image, clear_output, HTML # for interacting with the notebook better\n",
    "\n",
    "import numpy as np # matrix operations (ie. difference between two matricies)\n",
    "import cv2 # (OpenCV) computer vision functions (ie. tracking)\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "print('OpenCV Version: {}.{}.{}'.format(major_ver, minor_ver, subminor_ver))\n",
    "\n",
    "import matplotlib.pyplot as plt # (optional) for plotting and showing images inline\n",
    "%matplotlib inline\n",
    "\n",
    "import keras # high level api to tensorflow (or theano, CNTK, etc.) and useful image preprocessing\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "print('Keras image data format: {}'.format(K.image_data_format()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Constants*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_FOLDER = os.path.join('images') # images for visuals\n",
    "\n",
    "MODEL_PATH = os.path.join('model')\n",
    "MODEL_FILE = os.path.join(MODEL_PATH, 'hand_model_gray.hdf5') # path to model weights and architechture file\n",
    "MODEL_HISTORY = os.path.join(MODEL_PATH, 'model_history.txt') # path to model training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tools*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix colour channel ordering for plotting\n",
    "\n",
    "Matplotlib and OpenCV order colour channels in image matricies slightly differently, RGB and BGR respectively. We need to reorder the colour channels if we want to plot OpenCV images with Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgrtorgb(image):\n",
    "    return cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(name):\n",
    "    \"\"\"\n",
    "    Showing image files.\n",
    "    \"\"\"\n",
    "    fname = os.path.join(IMAGES_FOLDER, name)\n",
    "    ipydisplay(Image(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, figsize=(8,8), recolour=False):\n",
    "    \"\"\"\n",
    "    Plotting image matricies.\n",
    "    \"\"\"\n",
    "    if recolour: image = bgrtorgb(image)\n",
    "    plt.figure(figsize=figsize)\n",
    "    if image.shape[-1] == 3:\n",
    "        plt.imshow(image)\n",
    "    elif image.shape[-1] == 1 or len(image.shape) == 2:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        raise Exception(\"Image has invalid shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take webcam shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File nameaa\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        # Frame not successfully read from video capture\n",
    "        break\n",
    "        \n",
    "    # Display result\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27:# escape pressed \n",
    "        break\n",
    "    elif k == 115: # s pressed\n",
    "        fname = input(\"File name\")\n",
    "        cv2.imwrite(os.path.join(IMAGES_FOLDER, '{}.jpg'.format(fname)), frame)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading video from webcam\n",
    "\n",
    "Create an instance of a video capture object. Read and display frame by frame in a while loop until escape is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        # Frame not successfully read from video capture\n",
    "        break\n",
    "        \n",
    "    # Display result\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27: break # ESC pressed\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean windows and release video capture\n",
    "\n",
    "This is useful for when a video capture loop errors out, but your webcam is still stuck capturing and windows are left open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting key presses\n",
    "\n",
    "It's sometimes useful to get key presses to alter the algorithms being run, stat certain processes, run functions, etc.\n",
    "\n",
    "The video window must be selected to register a key press. *k*, in this case, will equal 255 if nothing is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Placeholder frame to show window\n",
    "    window = np.zeros((300,300))\n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"frame\", window)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k != 255: # A key is pressed\n",
    "        print(k)\n",
    "        \n",
    "    if k == 27:# escape pressed \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Computer Vision Concepts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What computers see\n",
    "\n",
    "Computers 'look' at images as multidimensional arrays or matricies but they can also be treated like functions (ex. taking the derivative over an image's x-axis).\n",
    "\n",
    "Below an image is loaded from the file system and loaded into memory. This matrix is 387 x 600 x 3 which represents the number of rows x number of columns x number of colour channels (RGB/BGR).\n",
    "\n",
    "We can then plot that data to view the image.\n",
    "\n",
    "Note: When images are loaded in OpenCV, they return BGR (blue, green, red) channels, where as matplotlib expects RGB (red, green, blue). Therefore, we need  to convert the loaded image matrix from BGR to RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "earth_fname = os.path.join(IMAGES_FOLDER, 'earth.jpg')\n",
    "earth_img = cv2.imread(earth_fname)\n",
    "# comment out the line below to see the colour difference\n",
    "earth_img = cv2.cvtColor(earth_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(earth_img)\n",
    "\n",
    "print('Image Shape: ', earth_img.shape, '\\n\\n')\n",
    "print('Image Matrix: \\n', earth_img, '\\n\\n')\n",
    "print('Image Plotted:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Filters and Functions\n",
    "\n",
    "Many times, images contain complex information that isn't need for a computation or reduces the speed of computation without much value added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blurring\n",
    "\n",
    "Blurring is useful when there is noise in an image you want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_img = earth_img.copy()\n",
    "blur_img = cv2.GaussianBlur(blur_img, (41, 41), 10)\n",
    "plt.imshow(blur_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilating\n",
    "\n",
    "Dilation, as it sounds, dilates pixel neighbourhoods by finding maximums over the image by the kernel size given. This is useful for expanding selections (we'll look at this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilate_img = earth_img.copy()\n",
    "dilate_img = cv2.dilate(dilate_img, np.ones((10,10), dtype=np.uint8), iterations=1)\n",
    "plt.imshow(dilate_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erosion\n",
    "\n",
    "Erosion is the opposite of dilation, useful for remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "erosion_img = earth_img.copy()\n",
    "erosion_img = cv2.erode(erosion_img, np.ones((10,10), dtype=np.uint8), iterations=1)\n",
    "plt.imshow(erosion_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canny edge detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "canny_img = earth_img.copy()\n",
    "canny_img = cv2.erode(canny_img, np.ones((8,8), dtype=np.uint8), iterations=1)\n",
    "edges = cv2.Canny(canny_img,100,100)\n",
    "plt.imshow(edges.astype(np.uint8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "\n",
    "Thresholding can be thought of as a function applied to each pixel of an image. This function takes a min and max thresholding values and if the pixel value falls in this range, it will 'return' the pixel, if not it will 'return' a black pixel.\n",
    "\n",
    "Generally, thresholding is applied to a greyscale image, but may also be applied to colour images, following a similair principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh_img = earth_img.copy()\n",
    "thresh_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(thresh_img, 80, 255, cv2.THRESH_BINARY)\n",
    "plt.imshow(thresh, cmap='gray')\n",
    "print(thresh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a background image to find differences (can be used for images and video)\n",
    "\n",
    "This technique requires a background image to find the difference between the background and the current frame to find what as changed. This difference creates a 'mask' that represents where in the image the foreground is. A draw back of this algorithm is that any movement of the camera, change of lighting, change in focus, etc. will make the current frame totally different from the background image.\n",
    "\n",
    "The algorithm:\n",
    "* load in the background image and the current frame\n",
    "* find the absolute difference between the images\n",
    "* create a mask that contains a 'map' of pixels that should be 'on or off'\n",
    "* apply the mask to the current frame to extract the foreground by iterating over each pixel and copying all pixels from the current frame that should be part of the foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bg_img = cv2.imread(os.path.join(IMAGES_FOLDER, 'bg.jpg'))\n",
    "current_frame_img = cv2.imread(os.path.join(IMAGES_FOLDER, 'current_frame.jpg'))\n",
    "\n",
    "diff = cv2.absdiff(bg_img, current_frame_img)\n",
    "mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "th, mask_thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "mask_indexes = mask_thresh > 0\n",
    "\n",
    "foreground = np.zeros_like(current_frame_img, dtype=np.uint8)\n",
    "for i, row in enumerate(mask_indexes):\n",
    "    foreground[i, row] = current_frame_img[i, row]\n",
    "\n",
    "plot_image(bg_img, recolour=True)\n",
    "plot_image(current_frame_img, recolour=True)\n",
    "plot_image(diff, recolour=True)\n",
    "plot_image(mask)\n",
    "plot_image(mask_thresh)\n",
    "plot_image(foreground, recolour=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using motion based background subtraction algorithms (mainly video)\n",
    "\n",
    "These algorithms are most used for video. The algorithm looks at a series of frames and computes which pixels are most static and identifies the foreground by the pixels that are moving. The MOG2 and KNN background subtractors are two different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERODE = True\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "# fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "video = cv2.VideoCapture(os.path.join(IMAGES_FOLDER, 'bg_subtract_movement.mp4'))\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.025)\n",
    "    \n",
    "    timer = cv2.getTickCount()\n",
    "    \n",
    "    # Read a new frame\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        # Frame not successfully read from video capture\n",
    "        break\n",
    "        \n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    # Apply erosion to clean up noise\n",
    "    if ERODE:\n",
    "        fgmask = cv2.erode(fgmask, np.ones((3,3), dtype=np.uint8), iterations=1)\n",
    "    \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(fgmask, \"FPS : \" + str(int(fps)), (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 255, 255), 2)\n",
    " \n",
    "    # Display result\n",
    "    cv2.imshow(\"fgmask\", fgmask)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27: break # ESC pressed\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours\n",
    "\n",
    "Finding contours is done by finding points or corners in an image and connecting those that have the same color or intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding contours and sorting contours\n",
    "\n",
    "Here we sort the contours by area and get the 4 largest contours, but we can also find all contours that are greater than a certain size. We can also fill contours by passing -1 to the last parameter of cv2.drawContours().\n",
    "\n",
    "You can also use contours for masking similair to how it was done above for background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLINE = True\n",
    "LRG_ONLY = True\n",
    "\n",
    "# window to hold the trackbar\n",
    "img = np.zeros((300,512,3), np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "# create trackbar\n",
    "cv2.createTrackbar('Thresh', 'image', 0, 255, lambda x: None)\n",
    "\n",
    "earth_fname = os.path.join(IMAGES_FOLDER, 'earth.jpg')\n",
    "earth_img = cv2.imread(earth_fname)\n",
    "\n",
    "while True:\n",
    "    thresh_min = cv2.getTrackbarPos('Thresh','image')\n",
    "    \n",
    "    contour_img = earth_img.copy()\n",
    "    contour_img = cv2.cvtColor(contour_img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, contour_img_thresh = cv2.threshold(contour_img, thresh_min, 255, 0)\n",
    "    contours, hierarchy = cv2.findContours(contour_img_thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if LRG_ONLY:\n",
    "        cnts = [x for x in contours if cv2.contourArea(x) > 20000]\n",
    "    else:\n",
    "        cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:3]\n",
    "\n",
    "    if OUTLINE:\n",
    "        # Draw only outlines\n",
    "        contour_img_display = cv2.drawContours(earth_img.copy(), cnts, -1, (238, 255, 0), 2)\n",
    "    else:\n",
    "        # Draw filled contours\n",
    "        contour_img_display = cv2.drawContours(earth_img.copy(), cnts, -1, (238, 255, 0), -1)\n",
    "\n",
    "    contour_img_display = cv2.cvtColor(contour_img_display, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    cv2.imshow('image', contour_img_display)\n",
    "    cv2.imshow('thresh', contour_img_thresh)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27: break # ESC pressed\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()    \n",
    "# plot_image(contour_img_thresh)\n",
    "# plot_image(contour_img_display, recolour=True)Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking\n",
    "\n",
    "Tracking is a very complex topic and we will simply use OpenCV's tracking algorithms to track objects. For this tutorial, we will use the Kernelized Correlation Filters (KCF) tracking as it performs well and provides built in tracking error detection.\n",
    "\n",
    "Available tracking algorithms:\n",
    "* MIL\n",
    "* BOOSTING\n",
    "* MEDIANFLOW\n",
    "* TLD\n",
    "* KCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tracker.\n",
    "def setup_tracker(ttype):\n",
    "    tracker_types = ['BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN']\n",
    "    tracker_type = tracker_types[ttype]\n",
    "\n",
    "    if tracker_type == 'BOOSTING':\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    if tracker_type == 'MIL':\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    if tracker_type == 'KCF':\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    if tracker_type == 'TLD':\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    if tracker_type == 'MEDIANFLOW':\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    if tracker_type == 'GOTURN':\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    \n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(os.path.join(IMAGES_FOLDER, 'moving_subject_scale.mp4'))\n",
    "\n",
    "# Read first frame\n",
    "success, frame = video.read()\n",
    "if not success:\n",
    "    print(\"first frame not read\")\n",
    "    sys.exit()\n",
    "\n",
    "tracker = setup_tracker(4)\n",
    "\n",
    "# Select roi for bbox\n",
    "bbox = cv2.selectROI(frame, False)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Initialize tracker with first frame and bounding box\n",
    "tracking_success = tracker.init(frame, bbox)\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.02)\n",
    "    \n",
    "    timer = cv2.getTickCount()\n",
    "    \n",
    "    # Read a new frame\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        # Frame not successfully read from video capture\n",
    "        break\n",
    "        \n",
    "    # Update tracker\n",
    "    tracking_success, bbox = tracker.update(frame)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    if tracking_success:\n",
    "        # Tracking success\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)        \n",
    "    else:\n",
    "        # Tracking failure\n",
    "        cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(frame, \"FPS : \" + str(int(fps)), (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27: break # ESC pressed\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (Deep) Neural Networks (NN/DNN)\n",
    "\n",
    "To get a taste of neural networks, we're going to teach one how to add two numbers between 0 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(os.path.join('nnslides', 'slide8.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(os.path.join('nnslides', 'slide11.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image(\"neuralnetwork.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build our neural network and compile it with a loss function and optimizers of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Sequential()\n",
    "\n",
    "test_model.add(Dense(2, input_shape=(2,), activation='relu'))\n",
    "test_model.add(Dense(32, activation='relu'))\n",
    "test_model.add(Dense(32, activation='relu'))\n",
    "test_model.add(Dense(1, activation='relu'))\n",
    "\n",
    "test_model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice look at the model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate some data for the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# list of 5000 tuples, each with two numbers between 0 and 5000\n",
    "data_x = [(random.randint(0, 5000), random.randint(0, 5000)) for x in range(5000)]\n",
    "# list of the expected result when the two numbers are added\n",
    "data_y = [x[0] + x[1] for x in data_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_model.fit(data_x, data_y, batch_size=16, epochs=2000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.save(os.path.join(MODEL_PATH, 'adder_model.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have a (really bad) neural network calculator. What's really cool about this is that you can also add numbers the network has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_model(os.path.join(MODEL_PATH, 'adder_model.hdf5'), compile=False)\n",
    "a = input(\"First number: \")\n",
    "b = input(\"Second number: \")\n",
    "d = np.expand_dims(np.array([a, b]), axis=0)\n",
    "test_model.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deep) Convolutional Neural Networks (CNN/DCNN)\n",
    "\n",
    "CNNs are a type of neural network often used for image problems. CNNs and DCNNs contain layers called convolutional layers that behave differently from the usual fully connected layers that you see in neural networks.\n",
    "\n",
    "The images below come from https://brohrer.github.io/how_convolutional_neural_networks_work.html which offers a more in depth explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Regular\" neural networks are less effective when working with images as every pixel is considered separately when calculating probabilities. This means that a normal fully-connected neural network is not good at classifying images, for example, if the object moves in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image('cnn1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers learn to look for *features* that are found in an image of certain classes. This is useful because it means that early on in a computation the model is not looking for specific *things* but rather simpler features like edges and corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image('cnn2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look for these features, the convolutional layers learn *filters* that are applied accross the image using a sliding window. This window finds how similair the filter being applied and the region of the image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image('cnn3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early in the computation, the filters are looking for edges and corners, then evenutally uses these edges and corners to look for higher and higher level features. By the end of the computation, filters are being learned to look for very high level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image('cnn4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After features are extracted by the convolutional layers, this high level data is fed into a regular neural network which learns which features correspond to which outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image('cnn5.png')\n",
    "show_image('cnn6.png')\n",
    "show_image('cnn7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Demo: Gesture Recognition*\n",
    "### Objective\n",
    "Use computer vision to track the user's hand and recognize gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject\n",
    "First we'll extract the user from the background to make tracking more effective and ensure a more accurate gesture prediction.\n",
    "\n",
    "Here we take the first frame as the background frame (or reset the it by pressing 'r'). Then we use the frame difference method as previously described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for applying a mask to an array\n",
    "def mask_array(array, imask):\n",
    "    if array.shape[:2] != imask.shape:\n",
    "        raise Exception(\"Shapes of input and imask are incompatible\")\n",
    "    output = np.zeros_like(array, dtype=np.uint8)\n",
    "    for i, row in enumerate(imask):\n",
    "        output[i, row] = array[i, row]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Begin capturing video\n",
    "video = cv2.VideoCapture(0)\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Read first frame\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Cannot read video\")\n",
    "    sys.exit()\n",
    "# Use the first frame as an initial background frame\n",
    "bg = frame.copy()\n",
    "\n",
    "\n",
    "# Kernel for erosion and dilation of masks\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "\n",
    "# Capture, process, display loop    \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    # Start timer\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    \n",
    "    # Processing\n",
    "    # First find the absolute difference between the two images\n",
    "    diff = cv2.absdiff(bg, frame)\n",
    "    mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the mask\n",
    "    th, thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Opening, closing and dilation\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img_dilation = cv2.dilate(closing, kernel, iterations=2)\n",
    "    # Get mask indexes\n",
    "    imask = img_dilation > 0\n",
    "    # Get foreground from mask\n",
    "    foreground = mask_array(frame, imask)\n",
    "    \n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(frame, \"FPS : \" + str(int(fps)), (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    \n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    # Display diff\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "    # Display thresh\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    # Display mask\n",
    "    cv2.imshow(\"img_dilation\", img_dilation)\n",
    "    # Display foreground\n",
    "    cv2.imshow(\"foreground\", foreground)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27: break # ESC pressed\n",
    "    elif k == 114 or k == 112: \n",
    "        # r pressed\n",
    "        bg = frame.copy()\n",
    "    elif k != 255: print(k)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking the hand\n",
    "    \n",
    "Display a bounding box and allow user to hit 't' to begin tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for applying a mask to an array\n",
    "def mask_array(array, imask):\n",
    "    if array.shape[:2] != imask.shape:\n",
    "        raise Exception(\"Shapes of input and imask are incompatible\")\n",
    "    output = np.zeros_like(array, dtype=np.uint8)\n",
    "    for i, row in enumerate(imask):\n",
    "        output[i, row] = array[i, row]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Begin capturing video\n",
    "video = cv2.VideoCapture(0)\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Read first frame\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Cannot read video\")\n",
    "    sys.exit()\n",
    "# Use the first frame as an initial background frame\n",
    "bg = frame.copy()\n",
    "\n",
    "\n",
    "# Kernel for erosion and dilation of masks\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "\n",
    "# Tracking\n",
    "# Bounding box -> (TopRightX, TopRightY, Width, Height)\n",
    "bbox_initial = (60, 60, 170, 170)\n",
    "bbox = bbox_initial\n",
    "# Tracking status, -1 for not tracking, 0 for unsuccessful tracking, 1 for successful tracking\n",
    "tracking = -1\n",
    "\n",
    "\n",
    "# Text display positions\n",
    "positions = {\n",
    "    'fps': (15, 20)\n",
    "}\n",
    "\n",
    "\n",
    "# Capture, process, display loop    \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    display = frame.copy()\n",
    "    if not ok:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    # Start timer\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    \n",
    "    # Processing\n",
    "    # First find the absolute difference between the two images\n",
    "    diff = cv2.absdiff(bg, frame)\n",
    "    mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the mask\n",
    "    th, thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Opening, closing and dilation\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img_dilation = cv2.dilate(closing, kernel, iterations=2)\n",
    "    # Get mask indexes\n",
    "    imask = img_dilation > 0\n",
    "    # Get foreground from mask\n",
    "    foreground = mask_array(frame, imask)\n",
    "    foreground_display = foreground.copy()\n",
    "    \n",
    "    \n",
    "    # If tracking is active, update the tracker\n",
    "    if tracking != -1:\n",
    "        tracking, bbox = tracker.update(foreground)\n",
    "        tracking = int(tracking)\n",
    "        \n",
    "        \n",
    "    # Use numpy array indexing to crop the hand\n",
    "    hand_crop = frame[int(bbox[1]):int(bbox[1]+bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]\n",
    "    \n",
    "        \n",
    "    # Draw bounding box\n",
    "    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "    cv2.rectangle(foreground_display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    cv2.rectangle(display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    \n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(foreground_display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    cv2.putText(display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    \n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"display\", display)\n",
    "    # Display diff\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "    # Display thresh\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    # Display mask\n",
    "    cv2.imshow(\"img_dilation\", img_dilation)\n",
    "    try:\n",
    "        # Display hand_crop\n",
    "        cv2.imshow(\"hand_crop\", hand_crop)\n",
    "    except:\n",
    "        pass\n",
    "    # Display foreground_display\n",
    "    cv2.imshow(\"foreground_display\", foreground_display)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    \n",
    "    if k == 27: break # ESC pressed\n",
    "    elif k == 114 or k == 112: \n",
    "        # r pressed\n",
    "        bg = frame.copy()\n",
    "        bbox = bbox_initial\n",
    "        tracking = -1\n",
    "    elif k == 116:\n",
    "        # t pressed\n",
    "        # Initialize tracker with first frame and bounding box\n",
    "        tracker = setup_tracker(2)\n",
    "        tracking = tracker.init(frame, bbox)\n",
    "    elif k == 115:\n",
    "        # s pressed\n",
    "        fname = os.path.join(\"data\", CURR_POS, \"{}_{}.jpg\".format(CURR_POS, get_unique_name(os.path.join(\"data\", CURR_POS))))\n",
    "        cv2.imwrite(fname, hand_crop)\n",
    "    elif k != 255: print(k)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data\n",
    "\n",
    "The next objective is to recognize which gesture a hand is posed in. We will train our neural network on 4 gestures: fist, five, point and swing. For this network we will train the network on the mask to reduce dimensionality. Doing this makes it a more simple problem for the network to model, while sacrificing information stored in the colours of an image.\n",
    "\n",
    "Here, we track our hand with the background subtracted and thresholded. Everytime you hit 's' a screen capture of your cropped hand is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'fist',\n",
    "    1: 'five',\n",
    "    2: 'point',\n",
    "    3: 'swing'\n",
    "}\n",
    "\n",
    "CURR_POSE = 'swing'\n",
    "DATA = 'validation_data'\n",
    "\n",
    "# Helper function for applying a mask to an array\n",
    "def mask_array(array, imask):\n",
    "    if array.shape[:2] != imask.shape:\n",
    "        raise Exception(\"Shapes of input and imask are incompatible\")\n",
    "    output = np.zeros_like(array, dtype=np.uint8)\n",
    "    for i, row in enumerate(imask):\n",
    "        output[i, row] = array[i, row]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Begin capturing video\n",
    "video = cv2.VideoCapture(0)\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Read first frame\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Cannot read video\")\n",
    "    sys.exit()\n",
    "# Use the first frame as an initial background frame\n",
    "bg = frame.copy()\n",
    "\n",
    "\n",
    "# Kernel for erosion and dilation of masks\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "\n",
    "# Tracking\n",
    "# Bounding box -> (TopRightX, TopRightY, Width, Height)\n",
    "bbox_initial = (60, 60, 170, 170)\n",
    "bbox = bbox_initial\n",
    "# Tracking status, -1 for not tracking, 0 for unsuccessful tracking, 1 for successful tracking\n",
    "tracking = -1\n",
    "\n",
    "\n",
    "# Text display positions\n",
    "positions = {\n",
    "    'hand_pose': (15, 40),\n",
    "    'fps': (15, 20)\n",
    "}\n",
    "\n",
    "\n",
    "# Image count for file name\n",
    "img_count = 0\n",
    "\n",
    "# Capture, process, display loop    \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    display = frame.copy()\n",
    "    if not ok:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    # Start timer\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    \n",
    "    # Processing\n",
    "    # First find the absolute difference between the two images\n",
    "    diff = cv2.absdiff(bg, frame)\n",
    "    mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the mask\n",
    "    th, thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Opening, closing and dilation\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img_dilation = cv2.dilate(closing, kernel, iterations=2)\n",
    "    # Get mask indexes\n",
    "    imask = img_dilation > 0\n",
    "    # Get foreground from mask\n",
    "    foreground = mask_array(frame, imask)\n",
    "    foreground_display = foreground.copy()\n",
    "    \n",
    "    \n",
    "    # If tracking is active, update the tracker\n",
    "    if tracking != -1:\n",
    "        tracking, bbox = tracker.update(foreground)\n",
    "        tracking = int(tracking)\n",
    "        \n",
    "        \n",
    "    # Use numpy array indexing to crop the foreground frame\n",
    "    hand_crop = img_dilation[int(bbox[1]):int(bbox[1]+bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]\n",
    "    \n",
    "        \n",
    "    # Draw bounding box\n",
    "    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "    cv2.rectangle(foreground_display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    cv2.rectangle(display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    \n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(foreground_display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    cv2.putText(display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    \n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"display\", display)\n",
    "    # Display diff\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "    # Display thresh\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    # Display mask\n",
    "    cv2.imshow(\"img_dilation\", img_dilation)\n",
    "    try:\n",
    "        # Display hand_crop\n",
    "        cv2.imshow(\"hand_crop\", hand_crop)\n",
    "    except:\n",
    "        pass\n",
    "    # Display foreground_display\n",
    "    cv2.imshow(\"foreground_display\", foreground_display)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    \n",
    "    if k == 27: break # ESC pressed\n",
    "    elif k == 114 or k == 112: \n",
    "        # r pressed\n",
    "        bg = frame.copy()\n",
    "        bbox = bbox_initial\n",
    "        tracking = -1\n",
    "    elif k == 116:\n",
    "        # t pressed\n",
    "        # Initialize tracker with first frame and bounding box\n",
    "        tracker = setup_tracker(2)\n",
    "        tracking = tracker.init(frame, bbox)\n",
    "    elif k == 115:\n",
    "        # s pressed\n",
    "        img_count += 1\n",
    "        fname = os.path.join(DATA, CURR_POSE, \"{}_{}.jpg\".format(CURR_POSE, img_count))\n",
    "        cv2.imwrite(fname, hand_crop)\n",
    "    elif k != 255: print(k)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "Here we assemble the neural network with keras and compile it for training.\n",
    "\n",
    "This is a very simple convolutional neural network containing three convolutional and max pooling layers. After a tensor is passed through the convolutional layers, it is flatted into a vector and passed through the dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(54, 54, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Training\n",
    "\n",
    "Here we use the keras data generator to augment data. This loads data and applies certain transformations to it in order to improve generalization of the model.\n",
    "\n",
    "The ImageDataGenerator.flow_from_directory() is a convenience method that prepares classification data according to file directories.\n",
    "\n",
    "Training data is used to train the model to recognize gestures and validation data is used to verify that the model is not over fitting to the training data and the network is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(zoom_range=0.2, rotation_range=10)\n",
    "\n",
    "training_generator = training_datagen.flow_from_directory(\n",
    "    'training_data',\n",
    "    target_size=(54, 54),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'validation_data',\n",
    "    target_size=(54, 54),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('training_data/swing/swing_111.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in training_datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='images/preview', save_prefix='fist', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "\n",
    "Now we can train the model on the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    steps_per_epoch=2000 // batch_size,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=200 // batch_size,\n",
    "    workers=8,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Model History\n",
    "\n",
    "Reading in the model fitting output and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "with open(MODEL_HISTORY) as history_file:\n",
    "    history = history_file.read()\n",
    "\n",
    "data = {}\n",
    "\n",
    "data['acc'] = re.findall(' acc: ([0-9]+\\.[0-9]+)', history)\n",
    "data['loss'] = re.findall(' loss: ([0-9]+\\.[0-9]+)', history)\n",
    "data['val_acc'] = re.findall(' val_acc: ([0-9]+\\.[0-9]+)', history)\n",
    "data['val_loss'] = re.findall(' val_loss: ([0-9]+\\.[0-9]+)', history)\n",
    "\n",
    "for key, values in data.items():\n",
    "    for i, val in enumerate(values):\n",
    "        values[i] = float(val)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(data['loss'])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(data['acc'])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(data['val_loss'])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(data['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"hand_model_gray.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model = load_model(MODEL_FILE, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'fist',\n",
    "    1: 'five',\n",
    "    2: 'point',\n",
    "    3: 'swing'\n",
    "}\n",
    "\n",
    "# Set up tracker.\n",
    "def setup_tracker(ttype):\n",
    "    tracker_types = ['BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN']\n",
    "    tracker_type = tracker_types[ttype]\n",
    "\n",
    "    if int(minor_ver) < 3:\n",
    "        tracker = cv2.Tracker_create(tracker_type)\n",
    "    else:\n",
    "        if tracker_type == 'BOOSTING':\n",
    "            tracker = cv2.TrackerBoosting_create()\n",
    "        if tracker_type == 'MIL':\n",
    "            tracker = cv2.TrackerMIL_create()\n",
    "        if tracker_type == 'KCF':\n",
    "            tracker = cv2.TrackerKCF_create()\n",
    "        if tracker_type == 'TLD':\n",
    "            tracker = cv2.TrackerTLD_create()\n",
    "        if tracker_type == 'MEDIANFLOW':\n",
    "            tracker = cv2.TrackerMedianFlow_create()\n",
    "        if tracker_type == 'GOTURN':\n",
    "            tracker = cv2.TrackerGOTURN_create()\n",
    "    \n",
    "    return tracker\n",
    "\n",
    "# Helper function for applying a mask to an array\n",
    "def mask_array(array, imask):\n",
    "    if array.shape[:2] != imask.shape:\n",
    "        raise Exception(\"Shapes of input and imask are incompatible\")\n",
    "    output = np.zeros_like(array, dtype=np.uint8)\n",
    "    for i, row in enumerate(imask):\n",
    "        output[i, row] = array[i, row]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Begin capturing video\n",
    "video = cv2.VideoCapture(0)\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Read first frame\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Cannot read video\")\n",
    "    sys.exit()\n",
    "# Use the first frame as an initial background frame\n",
    "bg = frame.copy()\n",
    "\n",
    "\n",
    "# Kernel for erosion and dilation of masks\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "\n",
    "# Display positions (pixel coordinates)\n",
    "positions = {\n",
    "    'hand_pose': (15, 40), # hand pose text\n",
    "    'fps': (15, 20), # fps counter\n",
    "    'null_pos': (200, 200) # used as null point for mouse control\n",
    "}\n",
    "\n",
    "\n",
    "# Tracking\n",
    "# Bounding box -> (TopRightX, TopRightY, Width, Height)\n",
    "bbox_initial = (116, 116, 170, 170) # Starting position for bounding box\n",
    "bbox = bbox_initial\n",
    "# Tracking status, -1 for not tracking, 0 for unsuccessful tracking, 1 for successful tracking\n",
    "tracking = -1\n",
    "\n",
    "\n",
    "# Capture, process, display loop    \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    display = frame.copy()\n",
    "    data_display = np.zeros_like(display, dtype=np.uint8) # Black screen to display data\n",
    "    if not ok:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    # Start timer\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    \n",
    "    # Processing\n",
    "    # First find the absolute difference between the two images\n",
    "    diff = cv2.absdiff(bg, frame)\n",
    "    mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the mask\n",
    "    th, thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Opening, closing and dilation\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img_dilation = cv2.dilate(closing, kernel, iterations=2)\n",
    "    # Get mask indexes\n",
    "    imask = img_dilation > 0\n",
    "    # Get foreground from mask\n",
    "    foreground = mask_array(frame, imask)\n",
    "    foreground_display = foreground.copy()\n",
    "    \n",
    "    \n",
    "    # If tracking is active, update the tracker\n",
    "    if tracking != -1:\n",
    "        tracking, bbox = tracker.update(foreground)\n",
    "        tracking = int(tracking)\n",
    "        \n",
    "        \n",
    "    # Use numpy array indexing to crop the foreground frame\n",
    "    hand_crop = img_dilation[int(bbox[1]):int(bbox[1]+bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]\n",
    "    try:\n",
    "        # Resize cropped hand and make prediction on gesture\n",
    "        hand_crop_resized = np.expand_dims(cv2.resize(hand_crop, (54, 54)), axis=0).reshape((1, 54, 54, 1))\n",
    "        prediction = hand_model.predict(hand_crop_resized)\n",
    "        predi = prediction[0].argmax() # Get the index of the greatest confidence\n",
    "        gesture = classes[predi]\n",
    "        \n",
    "        for i, pred in enumerate(prediction[0]):\n",
    "            # Draw confidence bar for each gesture\n",
    "            barx = positions['hand_pose'][0]\n",
    "            bary = 60 + i*60\n",
    "            bar_height = 20\n",
    "            bar_length = int(400 * pred) + barx # calculate length of confidence bar\n",
    "            \n",
    "            # Make the most confidence prediction green\n",
    "            if i == predi:\n",
    "                colour = (0, 255, 0)\n",
    "            else:\n",
    "                colour = (0, 0, 255)\n",
    "            \n",
    "            cv2.putText(data_display, \"{}: {}\".format(classes[i], pred), (positions['hand_pose'][0], 30 + i*60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
    "            cv2.rectangle(data_display, (barx, bary), (bar_length, bary - bar_height), colour, -1, 1)\n",
    "        \n",
    "        cv2.putText(display, \"hand pose: {}\".format(gesture), positions['hand_pose'], cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        cv2.putText(foreground_display, \"hand pose: {}\".format(gesture), positions['hand_pose'], cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    except Exception as ex:\n",
    "        cv2.putText(display, \"hand pose: error\", positions['hand_pose'], cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        cv2.putText(foreground_display, \"hand pose: error\", positions['hand_pose'], cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    \n",
    "        \n",
    "    # Draw bounding box\n",
    "    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "    cv2.rectangle(foreground_display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    cv2.rectangle(display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    \n",
    "    \n",
    "    # Move the mouse\n",
    "    hand_pos = ((p1[0] + p2[0])//2, (p1[1] + p2[1])//2)\n",
    "    mouse_change = ((p1[0] + p2[0])//2 - positions['null_pos'][0], positions['null_pos'][0] - (p1[1] + p2[1])//2)\n",
    "    # Draw mouse points\n",
    "    cv2.circle(display, positions['null_pos'], 5, (0,0,255), -1)\n",
    "    cv2.circle(display, hand_pos, 5, (0,255,0), -1)\n",
    "    cv2.line(display,positions['null_pos'],hand_pos,(255,0,0),5)\n",
    "    \n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(foreground_display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    cv2.putText(display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    \n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"display\", display)\n",
    "    # Display result\n",
    "    cv2.imshow(\"data\", data_display)\n",
    "    # Display diff\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "    # Display thresh\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    # Display mask\n",
    "    cv2.imshow(\"img_dilation\", img_dilation)\n",
    "    try:\n",
    "        # Display hand_crop\n",
    "        cv2.imshow(\"hand_crop\", hand_crop)\n",
    "    except:\n",
    "        pass\n",
    "    # Display foreground_display\n",
    "    cv2.imshow(\"foreground_display\", foreground_display)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    \n",
    "    if k == 27: break # ESC pressed\n",
    "    elif k == 114 or k == 108: \n",
    "        # r pressed\n",
    "        bg = frame.copy()\n",
    "        bbox = bbox_initial\n",
    "        tracking = -1\n",
    "    elif k == 116:\n",
    "        # t pressed\n",
    "        # Initialize tracker with first frame and bounding box\n",
    "        tracker = setup_tracker(2)\n",
    "        tracking = tracker.init(frame, bbox)\n",
    "    elif k == 115:\n",
    "        # s pressed\n",
    "        fname = os.path.join(\"data\", CURR_POS, \"{}_{}.jpg\".format(CURR_POS, get_unique_name(os.path.join(\"data\", CURR_POS))))\n",
    "        cv2.imwrite(fname, hand_crop)\n",
    "    elif k != 255: print(k)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo video for those who cannot run it\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IJV11OGTNT8\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jrobchin_Computer-Vision-Basics-with-Python-Keras-and-OpenCV",
   "language": "python",
   "name": "jrobchin_computer-vision-basics-with-python-keras-and-opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
