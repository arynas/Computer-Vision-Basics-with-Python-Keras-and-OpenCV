{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV Version: 4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad value in file '/Users/arynas/.matplotlib/matplotlibrc', line 1 ('backend: TkAggy'): Key backend: 'tkaggy' is not a valid value for backend; supported values are ['GTK3Agg', 'GTK3Cairo', 'GTK4Agg', 'GTK4Cairo', 'MacOSX', 'nbAgg', 'QtAgg', 'QtCairo', 'Qt5Agg', 'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras image data format: channels_last\n"
     ]
    }
   ],
   "source": [
    "import sys # system functions (ie. exiting the program)\n",
    "import os # operating system functions (ie. path building on Windows vs. MacOs)\n",
    "import time # for time operations\n",
    "import uuid # for generating unique file names\n",
    "import math # math functions\n",
    "\n",
    "from IPython.display import display as ipydisplay, Image, clear_output, HTML # for interacting with the notebook better\n",
    "\n",
    "import numpy as np # matrix operations (ie. difference between two matricies)\n",
    "import cv2 # (OpenCV) computer vision functions (ie. tracking)\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "print('OpenCV Version: {}.{}.{}'.format(major_ver, minor_ver, subminor_ver))\n",
    "\n",
    "import matplotlib.pyplot as plt # (optional) for plotting and showing images inline\n",
    "%matplotlib inline\n",
    "\n",
    "import keras # high level api to tensorflow (or theano, CNTK, etc.) and useful image preprocessing\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "print('Keras image data format: {}'.format(K.image_data_format()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Constants*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES_FOLDER = os.path.join('images') # images for visuals\n",
    "\n",
    "# MODEL_PATH = os.path.join('model')\n",
    "# MODEL_FILE = os.path.join(MODEL_PATH, 'hand_model_gray.hdf5') # path to model weights and architechture file\n",
    "# MODEL_HISTORY = os.path.join(MODEL_PATH, 'model_history.txt') # path to model training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tools*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix colour channel ordering for plotting\n",
    "\n",
    "Matplotlib and OpenCV order colour channels in image matricies slightly differently, RGB and BGR respectively. We need to reorder the colour channels if we want to plot OpenCV images with Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bgrtorgb(image):\n",
    "#     return cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_image(name):\n",
    "#     \"\"\"\n",
    "#     Showing image files.\n",
    "#     \"\"\"\n",
    "#     fname = os.path.join(IMAGES_FOLDER, name)\n",
    "#     ipydisplay(Image(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_image(image, figsize=(8,8), recolour=False):\n",
    "#     \"\"\"\n",
    "#     Plotting image matricies.\n",
    "#     \"\"\"\n",
    "#     if recolour: image = bgrtorgb(image)\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     if image.shape[-1] == 3:\n",
    "#         plt.imshow(image)\n",
    "#     elif image.shape[-1] == 1 or len(image.shape) == 2:\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#     else:\n",
    "#         raise Exception(\"Image has invalid shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Computer Vision Concepts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What computers see\n",
    "\n",
    "Computers 'look' at images as multidimensional arrays or matricies but they can also be treated like functions (ex. taking the derivative over an image's x-axis).\n",
    "\n",
    "Below an image is loaded from the file system and loaded into memory. This matrix is 387 x 600 x 3 which represents the number of rows x number of columns x number of colour channels (RGB/BGR).\n",
    "\n",
    "We can then plot that data to view the image.\n",
    "\n",
    "Note: When images are loaded in OpenCV, they return BGR (blue, green, red) channels, where as matplotlib expects RGB (red, green, blue). Therefore, we need  to convert the loaded image matrix from BGR to RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# earth_fname = os.path.join(IMAGES_FOLDER, 'earth.jpg')\n",
    "# earth_img = cv2.imread(earth_fname)\n",
    "# # comment out the line below to see the colour difference\n",
    "# earth_img = cv2.cvtColor(earth_img, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(earth_img)\n",
    "\n",
    "# print('Image Shape: ', earth_img.shape, '\\n\\n')\n",
    "# print('Image Matrix: \\n', earth_img, '\\n\\n')\n",
    "# print('Image Plotted:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Filters and Functions\n",
    "\n",
    "Many times, images contain complex information that isn't need for a computation or reduces the speed of computation without much value added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blurring\n",
    "\n",
    "\n",
    "Blurring is useful when there is noise in an image you want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_blur_img = earth_img.copy()\n",
    "# box_blur_img = cv2.blur(box_blur_img, (41, 41))\n",
    "# plt.imshow(box_blur_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blur_img = earth_img.copy()\n",
    "# blur_img = cv2.GaussianBlur(blur_img, (41, 41), 10)\n",
    "# plt.imshow(blur_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilating\n",
    "\n",
    "Dilation, as it sounds, dilates pixel neighbourhoods by finding maximums over the image by the kernel size given. This is useful for expanding selections (we'll look at this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dilate_img = earth_img.copy()\n",
    "# dilate_img = cv2.dilate(dilate_img, np.ones((10,10), dtype=np.uint8), iterations=1)\n",
    "# plt.imshow(dilate_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erosion\n",
    "\n",
    "Erosion is the opposite of dilation, useful for remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# erosion_img = earth_img.copy()\n",
    "# erosion_img = cv2.erode(erosion_img, np.ones((10,10), dtype=np.uint8), iterations=1)\n",
    "# plt.imshow(erosion_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canny edge detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# canny_img = earth_img.copy()\n",
    "# canny_img = cv2.erode(canny_img, np.ones((12,12), dtype=np.uint8), iterations=1)\n",
    "# thresh = 75\n",
    "# edges = cv2.Canny(canny_img,thresh,thresh)\n",
    "# plt.imshow(edges.astype(np.uint8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "\n",
    "Thresholding can be thought of as a function applied to each pixel of an image. This function takes a min and max thresholding values and if the pixel value falls in this range, it will 'return' the pixel, if not it will 'return' a black pixel.\n",
    "\n",
    "Generally, thresholding is applied to a greyscale image, but may also be applied to colour images, following a similair principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# thresh_img = earth_img.copy()\n",
    "# thresh_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2GRAY)\n",
    "# ret, thresh = cv2.threshold(thresh_img, 80, 255, cv2.THRESH_BINARY)\n",
    "# plt.imshow(thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_img = earth_img.copy()\n",
    "# thresh_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2GRAY)\n",
    "# ret, thresh = cv2.threshold(thresh_img, 155, 255, cv2.THRESH_BINARY)\n",
    "# plt.imshow(thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_img = earth_img.copy()\n",
    "# thresh_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2GRAY)\n",
    "# ret, thresh = cv2.threshold(thresh_img, 155, 170, cv2.THRESH_BINARY)\n",
    "# plt.imshow(thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a background image to find differences (can be used for images and video)\n",
    "\n",
    "This technique requires a background image to find the difference between the background and the current frame to find what as changed. This difference creates a 'mask' that represents where in the image the foreground is. A draw back of this algorithm is that any movement of the camera, change of lighting, change in focus, etc. will make the current frame totally different from the background image.\n",
    "\n",
    "The algorithm:\n",
    "* load in the background image and the current frame\n",
    "* find the absolute difference between the images\n",
    "* create a mask that contains a 'map' of pixels that should be 'on or off'\n",
    "* apply the mask to the current frame to extract the foreground by iterating over each pixel and copying all pixels from the current frame that should be part of the foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bg_img = cv2.imread(os.path.join(IMAGES_FOLDER, 'bg.jpg'))\n",
    "# current_frame_img = cv2.imread(os.path.join(IMAGES_FOLDER, 'current_frame.jpg'))\n",
    "\n",
    "# diff = cv2.absdiff(bg_img, current_frame_img)\n",
    "# mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "# th, mask_thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "# mask_blur = cv2.GaussianBlur(mask_thresh, (3, 3), 10)\n",
    "# mask_erosion = cv2.erode(mask_blur, np.ones((5,5), dtype=np.uint8), iterations=1)\n",
    "\n",
    "# mask_indexes = mask_erosion > 0\n",
    "\n",
    "# foreground = np.zeros_like(current_frame_img, dtype=np.uint8)\n",
    "# for i, row in enumerate(mask_indexes):\n",
    "#     foreground[i, row] = current_frame_img[i, row]\n",
    "\n",
    "# plot_image(bg_img, recolour=True)\n",
    "# plot_image(current_frame_img, recolour=True)\n",
    "# plot_image(diff, recolour=True)\n",
    "# plot_image(mask)\n",
    "# plot_image(mask_erosion)\n",
    "# plot_image(foreground, recolour=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using motion based background subtraction algorithms (mainly video)\n",
    "\n",
    "These algorithms are most used for video. The algorithm looks at a series of frames and computes which pixels are most static and identifies the foreground by the pixels that are moving. The MOG2 and KNN background subtractors are two different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERODE = True\n",
    "\n",
    "# fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "# # fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "# video = cv2.VideoCapture(os.path.join(IMAGES_FOLDER, 'bg_subtract_movement.mp4'))\n",
    "\n",
    "# while True:\n",
    "#     time.sleep(0.025)\n",
    "    \n",
    "#     timer = cv2.getTickCount()\n",
    "    \n",
    "#     # Read a new frame\n",
    "#     success, frame = video.read()\n",
    "#     if not success:\n",
    "#         # Frame not successfully read from video capture\n",
    "#         break\n",
    "        \n",
    "#     fgmask = fgbg.apply(frame)\n",
    "    \n",
    "#     # Apply erosion to clean up noise\n",
    "#     if ERODE:\n",
    "#         fgmask = cv2.erode(fgmask, np.ones((3,3), dtype=np.uint8), iterations=1)\n",
    "    \n",
    "#     # Calculate Frames per second (FPS)\n",
    "#     fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "#     # Display FPS on frame\n",
    "#     cv2.putText(fgmask, \"FPS : \" + str(int(fps)), (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 255, 255), 2)\n",
    " \n",
    "#     # Display result\n",
    "#     cv2.imshow(\"fgmask\", fgmask)\n",
    "    \n",
    "#     k = cv2.waitKey(1) & 0xff\n",
    "#     if k == 27: break # ESC pressed\n",
    "        \n",
    "# cv2.destroyAllWindows()\n",
    "# video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours\n",
    "\n",
    "Finding contours is done by finding points or corners in an image and connecting those that have the same color or intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding contours and sorting contours\n",
    "\n",
    "Here we sort the contours by area and get the 4 largest contours, but we can also find all contours that are greater than a certain size. We can also fill contours by passing -1 to the last parameter of cv2.drawContours().\n",
    "\n",
    "You can also use contours for masking similair to how it was done above for background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLINE = True\n",
    "# LRG_ONLY = True\n",
    "\n",
    "# # window to hold the trackbar\n",
    "# img = np.zeros((300,512,3), np.uint8)\n",
    "# cv2.namedWindow('image')\n",
    "\n",
    "# # create trackbar\n",
    "# cv2.createTrackbar('Thresh', 'image', 0, 255, lambda x: None)\n",
    "\n",
    "# earth_fname = os.path.join(IMAGES_FOLDER, 'earth.jpg')\n",
    "# earth_img = cv2.imread(earth_fname)\n",
    "\n",
    "# while True:\n",
    "#     thresh_min = cv2.getTrackbarPos('Thresh','image')\n",
    "    \n",
    "#     contour_img = earth_img.copy()\n",
    "#     contour_img = cv2.cvtColor(contour_img, cv2.COLOR_BGR2GRAY)\n",
    "#     ret, contour_img_thresh = cv2.threshold(contour_img, thresh_min, 255, 0)\n",
    "#     contours, hierarchy = cv2.findContours(contour_img_thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     if LRG_ONLY:\n",
    "#         cnts = [x for x in contours if cv2.contourArea(x) > 20000]\n",
    "#     else:\n",
    "#         cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:3]\n",
    "\n",
    "#     if OUTLINE:\n",
    "#         # Draw only outlines\n",
    "#         contour_img_display = cv2.drawContours(earth_img.copy(), cnts, -1, (238, 255, 0), 2)\n",
    "#     else:\n",
    "#         # Draw filled contours\n",
    "#         contour_img_display = cv2.drawContours(earth_img.copy(), cnts, -1, (238, 255, 0), -1)\n",
    "\n",
    "#     contour_img_display = cv2.cvtColor(contour_img_display, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     cv2.imshow('image', contour_img_display)\n",
    "#     cv2.imshow('thresh', contour_img_thresh)\n",
    "    \n",
    "#     k = cv2.waitKey(1) & 0xff\n",
    "#     if k == 27: break # ESC pressed\n",
    "        \n",
    "\n",
    "# cv2.destroyAllWindows()    \n",
    "# # plot_image(contour_img_thresh)\n",
    "# # plot_image(contour_img_display, recolour=True)Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking\n",
    "\n",
    "Tracking is a very complex topic and we will simply use OpenCV's tracking algorithms to track objects. For this tutorial, we will use the Kernelized Correlation Filters (KCF) tracking as it performs well and provides built in tracking error detection.\n",
    "\n",
    "Available tracking algorithms:\n",
    "* MIL\n",
    "* BOOSTING\n",
    "* MEDIANFLOW\n",
    "* TLD\n",
    "* KCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up tracker.\n",
    "# def setup_tracker(ttype):\n",
    "#     tracker_types = ['BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN']\n",
    "#     tracker_type = tracker_types[ttype]\n",
    "\n",
    "#     if tracker_type == 'BOOSTING':\n",
    "#         tracker = cv2.TrackerBoosting_create()\n",
    "#     if tracker_type == 'MIL':\n",
    "#         tracker = cv2.TrackerMIL_create()\n",
    "#     if tracker_type == 'KCF':\n",
    "#         tracker = cv2.TrackerKCF_create()\n",
    "#     if tracker_type == 'TLD':\n",
    "#         tracker = cv2.TrackerTLD_create()\n",
    "#     if tracker_type == 'MEDIANFLOW':\n",
    "#         tracker = cv2.TrackerMedianFlow_create()\n",
    "#     if tracker_type == 'GOTURN':\n",
    "#         tracker = cv2.TrackerGOTURN_create()\n",
    "    \n",
    "#     return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture(\"demos/images/moving_subject_scale.mp4\")\n",
    "# # video = cv2.VideoCapture(0)\n",
    "\n",
    "# # Read first frame\n",
    "# success, frame = video.read()\n",
    "# if not success:\n",
    "#     print(\"first frame not read\")\n",
    "#     sys.exit()\n",
    "\n",
    "# tracker = setup_tracker(4)\n",
    "\n",
    "# # Select roi for bbox\n",
    "# bbox = cv2.selectROI(frame, False)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Initialize tracker with first frame and bounding box\n",
    "# tracking_success = tracker.init(frame, bbox)\n",
    "\n",
    "# while True:\n",
    "#     time.sleep(0.02)\n",
    "    \n",
    "#     timer = cv2.getTickCount()\n",
    "    \n",
    "#     # Read a new frame\n",
    "#     success, frame = video.read()\n",
    "#     if not success:\n",
    "#         # Frame not successfully read from video capture\n",
    "#         break\n",
    "        \n",
    "#     # Update tracker\n",
    "#     tracking_success, bbox = tracker.update(frame)\n",
    "    \n",
    "#     # Draw bounding box\n",
    "#     if tracking_success:\n",
    "#         # Tracking success\n",
    "#         p1 = (int(bbox[0]), int(bbox[1]))\n",
    "#         p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "#         cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)        \n",
    "#     else:\n",
    "#         # Tracking failure\n",
    "#         cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "#     # Calculate Frames per second (FPS)\n",
    "#     fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "#     # Display FPS on frame\n",
    "#     cv2.putText(frame, \"FPS : \" + str(int(fps)), (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 0, 0), 2)\n",
    "    \n",
    "#     # Display result\n",
    "#     cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "#     k = cv2.waitKey(1) & 0xff\n",
    "#     if k == 27: break # ESC pressed\n",
    "        \n",
    "# cv2.destroyAllWindows()\n",
    "# video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data\n",
    "\n",
    "The next objective is to recognize which gesture a hand is posed in. We will train our neural network on 4 gestures: fist, five, point and swing. For this network we will train the network on the mask to reduce dimensionality. Doing this makes it a more simple problem for the network to model, while sacrificing information stored in the colours of an image.\n",
    "\n",
    "Here, we track our hand with the background subtracted and thresholded. Everytime you hit 's' a screen capture of your cropped hand is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'fist',\n",
    "    1: 'five',\n",
    "    2: 'point',\n",
    "    3: 'swing'\n",
    "}\n",
    "\n",
    "CURR_POSE = 'five'\n",
    "DATA = 'training_data'\n",
    "\n",
    "# Helper function for applying a mask to an array\n",
    "def mask_array(array, imask):\n",
    "    if array.shape[:2] != imask.shape:\n",
    "        raise Exception(\"Shapes of input and imask are incompatible\")\n",
    "    output = np.zeros_like(array, dtype=np.uint8)\n",
    "    for i, row in enumerate(imask):\n",
    "        output[i, row] = array[i, row]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Begin capturing video\n",
    "video = cv2.VideoCapture(1)\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Read first frame\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Cannot read video\")\n",
    "    sys.exit()\n",
    "# Use the first frame as an initial background frame\n",
    "bg = frame.copy()\n",
    "\n",
    "\n",
    "# Kernel for erosion and dilation of masks\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "\n",
    "# Tracking\n",
    "# Bounding box -> (TopRightX, TopRightY, Width, Height)\n",
    "bbox_initial = (60, 60, 170, 170)\n",
    "bbox = bbox_initial\n",
    "# Tracking status, -1 for not tracking, 0 for unsuccessful tracking, 1 for successful tracking\n",
    "tracking = -1\n",
    "\n",
    "\n",
    "# Text display positions\n",
    "positions = {\n",
    "    'hand_pose': (15, 40),\n",
    "    'fps': (15, 20)\n",
    "}\n",
    "\n",
    "\n",
    "# Image count for file name\n",
    "img_count = 0\n",
    "\n",
    "# Capture, process, display loop    \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ok, frame = video.read()\n",
    "    display = frame.copy()\n",
    "    if not ok:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    # Start timer\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    \n",
    "    # Processing\n",
    "    # First find the absolute difference between the two images\n",
    "    diff = cv2.absdiff(bg, frame)\n",
    "    mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold the mask\n",
    "    th, thresh = cv2.threshold(mask, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Opening, closing and dilation\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img_dilation = cv2.dilate(closing, kernel, iterations=2)\n",
    "    # Get mask indexes\n",
    "    imask = img_dilation > 0\n",
    "    # Get foreground from mask\n",
    "    foreground = mask_array(frame, imask)\n",
    "    foreground_display = foreground.copy()\n",
    "    \n",
    "    \n",
    "    # If tracking is active, update the tracker\n",
    "    if tracking != -1:\n",
    "        tracking, bbox = tracker.update(foreground)\n",
    "        tracking = int(tracking)\n",
    "        \n",
    "        \n",
    "    # Use numpy array indexing to crop the foreground frame\n",
    "    hand_crop = img_dilation[int(bbox[1]):int(bbox[1]+bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]\n",
    "    \n",
    "        \n",
    "    # Draw bounding box\n",
    "    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "    cv2.rectangle(foreground_display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    cv2.rectangle(display, p1, p2, (255, 0, 0), 2, 1)\n",
    "    \n",
    "        \n",
    "    # Calculate Frames per second (FPS)\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(foreground_display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    cv2.putText(display, \"FPS : \" + str(int(fps)), positions['fps'], cv2.FONT_HERSHEY_SIMPLEX, 0.65, (50, 170, 50), 2)\n",
    "    \n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow(\"display\", display)\n",
    "    # Display diff\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "    # Display thresh\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    # Display mask\n",
    "    cv2.imshow(\"img_dilation\", img_dilation)\n",
    "    try:\n",
    "        # Display hand_crop\n",
    "        cv2.imshow(\"hand_crop\", hand_crop)\n",
    "    except:\n",
    "        pass\n",
    "    # Display foreground_display\n",
    "    cv2.imshow(\"foreground_display\", foreground_display)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    \n",
    "    if k == 27: break # ESC pressed\n",
    "    elif k == 114 or k == 112: \n",
    "        # r pressed\n",
    "        bg = frame.copy()\n",
    "        bbox = bbox_initial\n",
    "        tracking = -1\n",
    "    elif k == 116:\n",
    "        # t pressed\n",
    "        # Initialize tracker with first frame and bounding box\n",
    "        tracker = setup_tracker(2)\n",
    "        tracking = tracker.init(frame, bbox)\n",
    "    elif k == 115:\n",
    "        # s pressed\n",
    "        img_count += 1\n",
    "        fname = os.path.join(DATA, CURR_POSE, \"{}_{}.jpg\".format(CURR_POSE, img_count))\n",
    "        print(fname, hand_crop)\n",
    "        cv2.imwrite(fname, hand_crop)\n",
    "        print()\n",
    "    elif k != 255: print(k)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jrobchin_Computer-Vision-Basics-with-Python-Keras-and-OpenCV",
   "language": "python",
   "name": "jrobchin_computer-vision-basics-with-python-keras-and-opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
